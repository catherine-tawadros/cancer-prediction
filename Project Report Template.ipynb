{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Report\"\n",
    "subtitle: Team name\n",
    "author: Sarah Abdulwahid, Christina Tzavara, Alex Olmeta, Catherine Tawadros, Charles Bugayer \n",
    "date: 06/07/2023\n",
    "number-sections: true\n",
    "abstract: _The ABSTRACT is to be in fully-justified italicized text at the top of the report, below the author information. The abstract section must summarise the problem statement, the developed model(s), the metric(s) optimized and the recommendations to the stakeholders based on the model (if any). You may also briefly mention any major EDA-based insights that helped develop the model or directly translated into recommendations to the stakeholders. However, the abstract must not be more than 200 words in length_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Background / Motivation\n",
    "\n",
    "What motivated you to work on this problem?\n",
    "\n",
    "Mention any background about the problem, if it is required to understand your analysis later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7df152b8",
   "metadata": {},
   "source": [
    "As data science students seeking entry into the data science field post-graduation, we were interested in creating a robust regression model to predict salaries. Due to the increasing demand for professionals in this industry and its rapid growth, it is important, for employees and employers alike, that salaries are accurately predicted to ensure fair and competitive compensation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ff1421",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "Describe your problem statement. Articulate your objectives using absolutely no jargon. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96d36cc6",
   "metadata": {},
   "source": [
    "Our aim was to develop a regression model to predict salaries in the data science field using information from different data science-related jobs all over the world. We created a regression model using different tuning techniques that can help ensure fair compensation for job seekers and aid organizations in attracting top talent by predicting salaries accurately. Our objective is to accurately estimate salaries, a continuous response variable, while optimizing Mean Absolute Error (MAE). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38c7b95f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "What data did you use? Provide details about your data. Include links to data if you are using open-access data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8534db08",
   "metadata": {},
   "source": [
    "Our dataset is from Kaggle and can be accessed through this link: AI & ML salaries.The dataset provides information on salaries for many job positions in the general data science field, such as Artificial Intelligence and Machine Learning engineers, among others. It captures changes from 2020 to 2023 and is updated weekly. In the form used in our project, the dataset has 4134 observations and 10 predictors. Some predictors include company size, experience level, employment type and remote ratio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c255035",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "Who cares? If you are successful, what difference will it make to them?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b09024e",
   "metadata": {},
   "source": [
    "Our main stakeholders for this project are current data scientists and machine learning practitioners along with job seekers in the data science space. For current data scientists, our predictions from salary data in the last three years can help them in making career decisions and negotiate salaries with their employers. For job seekers, our predictions can inform them so that they can appropriately evaluate their job offers and generally be aware of the future trajectory of specific subcategories in the Data Science & Machine Learning industry (compensation-wise), so that they can target their applications better. Other stakeholders for this project include the companies in the DS & ML space themselves, who can be informed through our predictions to effectively set competitive salary ranges. They can attract top talent by tailoring their compensation packages. Lastly, our insights are useful for universities and educational institutions interested in assessing the Return on Investment (ROI) of their data science programs and attract more candidates, as well as researchers, who can use our predictions to drive further research to assess industry growth in the data science field."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Show the distribution of the response here. Report the standard deviation and mean in case of a regression problem, and proportion of 0s and 1s in case of classification.\n",
    "\n",
    "For all other content, as mentioned below, just provide the highlights *(if any)* and put the details in the appendix.\n",
    "\n",
    "In a tabular form, show the distribution of values of each variable used in the analysis - for both categorical and continuous variables. Distribution of a categorical variable must include the number of missing values, the number of unique values, the frequency of all its levels. If a categorical variable has too many levels, you may just include the counts of the top 3-5 levels. \n",
    "\n",
    "Mention any useful insights you obtained from the data quality check that helped you develop the model or helped you realize the necessary data cleaning / preparation. Its ok if there were none.\n",
    "\n",
    "Were there any potentially incorrect values of variables that required cleaning? If yes, how did you clean them? Were there missing values? How did you handle them? Its ok if the data was already clean.\n",
    "\n",
    "Did you do any data wrangling or data preparation before the data was ready to use for model development? Did you create any new predictors from exisiting predictors? For example, if you have number of transactions and spend in a credit card dataset, you may create spend per transaction for predicting if a customer pays their credit card bill. Mention the steps at a broad level, you may put minor details in the appendix. Only mention the steps that ended up being useful towards developing your model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa42d56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='salary_in_usd', ylabel='Count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data['salary_in_usd'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78297b00",
   "metadata": {},
   "source": [
    "One of the predictors, job_title (string) had over 100 unique values. This made it very difficult for us to extract relevant information based on this column. Therefore, we binned each job title into a broader category of job titles. This wasn’t straightforward and did involve a lot of manual work beyond just binning. Based on the field, title, expertise, type of work, and common sense we developed a dictionary to put the jobs into 10 categories. This allowed us to better leverage job titles in our model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d43d554",
   "metadata": {},
   "source": [
    "One variable we had to drop was salary. Since we decided to predict salary_in_usd because it was standardized, we had to drop salary because of how highly correlated it was. It didn’t make any sense to use it as a predictor since it had a correlation of 0.99 with the response and using it would have defeated the point of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "If there is any EDA that helped with model development, put it here. If EDA didn't help then mention that, and you may show your EDA effort *(if any)* in the appendix.\n",
    "\n",
    "List the insights (as bullet points), if any, you got from EDA  that ended up being useful towards developing your final model. \n",
    "\n",
    "If there are too many plots / tables, you may put them into appendix, and just mention the insights you got from them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1f12d5d",
   "metadata": {},
   "source": [
    "Note that you can write code to publish the results of the code, but hide the code using the yaml setting `#|echo: false`. For example, the code below makes a plot, but the code itself is not published with Quarto in the report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fefc236",
   "metadata": {},
   "source": [
    "- No skew in target variable\n",
    "- Each predictor was important and uncorrelated with the others. \n",
    "- We found other insights but did not incorporate them into our models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "What kind of a models did you use? What performance metric(s) did you optimize and why?\n",
    "\n",
    "Is there anything unorthodox / new in your approach? \n",
    "\n",
    "What problems did you anticipate? What problems did you encounter? \n",
    "\n",
    "Did your problem already have solution(s) (posted on Kaggle or elsewhere). If yes, then how did you build upon those solutions, what did you do differently? Is your model better as compared to those solutions in terms of prediction accuracy or your chosen metric?\n",
    "\n",
    "**Important: Mention any code repositories (with citations) or other sources that you used, and specifically what changes you made to them for your project.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5882ae89",
   "metadata": {},
   "source": [
    "We decided to create individual base models using MARS, Decision Trees (with cost-complexity pruning), Random Forests, Gradient Boosting, and XGBoost. We then ensemble these models using voting and stacking regressor methods to see whether this would help reduce the bias and/or variance of our model, while optimizing our metric: Mean Absolute Error (MAE). \n",
    "We decided to optimize MAE, to allow for a balanced approach, while reducing both overestimation and underestimation errors. Additionally, MAE is more practical than Root Mean Squared Error (RMSE) in real-world scenarios as it helps stakeholders interpret the model's predictions directly, since MAE is in the same units as the target variable. For our dataset, the standard deviation of the response variable (salary in USD) is $63,605. \n",
    "We anticipated that the column names for job titles would be a problem since there is no standardized job title for our data set. To solve this problem, we decided to bin the original job titles into 10 main categories to enable more generalized conclusions and improve the interpretability of our findings. While doing EDA, we also encountered the problem of evaluating the differences between using the work year column as a feature to predict the salary or to perform a time-series analysis, which would entail predicting salaries for future work years using historical data. Although a time-series analysis would provide insights into future salary trends, it would require more advanced modeling techniques beyond the scope of the class.\n",
    "By choosing the work year as a feature, we can still capture important trends in our data while maintaining the feasibility of implementing our regression models within the confines of our current study. This approach enables us to develop accurate salary predictions without the additional complexities introduced by time-series modeling.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model: Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Every person must describe their hyperparameter tuning procedure. Show the grid of hyperparameter values over which the initial search was done *(you may paste your grid search / random search / any other search code)*, and the optimal hyperparameter values obtained. After getting the initial search results, how did you make decisions *(if any)* to further fine-tune your model. Did you do another grid / random search or did you tune hyperparameters sequentially? If you think you didn't need any fine tuning after the initial results, then mention that and explain why.\n",
    "\n",
    "Put each model in a section of its name and mention the name of the team-member tuning the model. Below is an example:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea422beb",
   "metadata": {},
   "source": [
    "### Mars\n",
    "*By Christina Tzavara*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e616b2a",
   "metadata": {},
   "source": [
    "To develop my first MARS model, I began with a coarse grid search to optimize the max_degree hyperparameter. I started with a range of (1,5) in terms of values for max_degree. I used the MAE as a scoring metric to see where the max_degree minimizes it.\n",
    "I utilized a visualization of the CV MAE against the max_degree to get informed on what value of max_degree minimizes the MAE: the optimal value for this hyperparameter with my initial search ended up being max_degree = 1. A default MARS model with max_degree = 1 yielded an MAE of $37,315.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Informed by the previous search, I decided to run a finer search for optimal max_degree, this time tuning this hyperparameter simultaneously with max_terms. I allowed for a range of (1,3) for the degree and (50,100,200) for max_terms. I wanted to be able to search across a sufficiently large range of models in terms of max_terms, to evaluate the performance of models with different complexities, while keeping the range reasonable enough to run simultaneously with the max_degree. The results of the second round of tuning was:\n",
    "\n",
    "The final MARS model yielded a lower MAE of $36,813 and can be shown below:\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b78fbd0e",
   "metadata": {},
   "source": [
    "### Decision Trees with Cost-Complexity Pruning\n",
    "*By Sarah Abdulwahid*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e301c58b",
   "metadata": {},
   "source": [
    "To begin modeling with decision trees, I created an untuned Decision Tree Regressor model and got the max_depth = 44 and max_leaf_nodes = 632. The MAE for this model was $37,495.\n",
    "Next, I used these outputs to determine hyper parameter ranges to consider for the tuning process using 5-fold cross validation. I considered the range [2,44] and [2, 632] and optimized for neg_mean_absolute_error. This search got me the optimal values of max_depth = 6 and max_leaf_nodes=42. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since the values were near the ends of the ranges I considered, I decided to perform a fine-grid search using 5-fold cross validation to improve the model performance. To narrow the range for hyperparameter tuning, I plotted max_depth and max_leaf_nodes against MAE (results from my first GridSearchCV). \n",
    "\n",
    "From these plots, I was able to narrow my range to [4,10] for max_leaves and [2,200] for max_leaf_nodes. \n",
    "\n",
    "From the results of the fine grid search I created a new Decision Tree Regressor with max_depth=6 and max_leaf_nodes = 42 as the parameters. This helped reduce the MAE to $36,561.\n",
    "To help prevent overfitting, I decided to perform cost-complexity pruning. I created a decision tree regressor model without any restrictions on its growth and then computed the cost complexity pruning path on the training data to determine the potential alpha values for pruning. \n",
    "Then, I tuned the alpha hyperparameter by performing a grid search on the potential alpha values using 5-fold cross validation and optimizing for MAE. I got the optimal ccp_alpha = 8824077.621291757.\n",
    "I used this value to create a new Decision Tree Regressor with this alpha as a parameter. This reduced the MAE to $36,258. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2916849c",
   "metadata": {},
   "source": [
    "### Bagged trees & Random forest\n",
    "*By Catherine Tawadros*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef541d57",
   "metadata": {},
   "source": [
    "Since I was particularly limited by my laptop’s runtime, I started with very vague searches and got increasingly more specific as time went on. I started with a very coarse search using RandomizedSearchCV and the following predictors:\n",
    "parameters = {'max_depth': np.arange(5,50,7), \"max_features\": np.arange(2,10,2), 'max_leaf_nodes': np.arange(2000,4000,500), 'bootstrap':[True], 'ccp_alpha': np.arange(0.05, .15, 0.05)}\n",
    "Receiving the following parameter output:\n",
    "{'max_leaf_nodes': 2000, 'max_features': 4, 'max_depth': 33, 'ccp_alpha': 0.05, 'bootstrap': True}\n",
    "Noticing that lower parameter values for max_leaf_nodes, max_features, and ccp_alpha were preferred, I ran the following RandomizedSearchCV:\n",
    "parameters = {'max_depth': np.arange(5,50,7), \"max_features\": np.arange(2,8,1), 'max_leaf_nodes': np.arange(2000,2500,100), 'bootstrap':[True], 'ccp_alpha': np.arange(0.025, .1, 0.025)}\n",
    "Receiving the following as a parameter output:\n",
    "{'max_leaf_nodes': 2400, 'max_features': 4, 'max_depth': 33, 'ccp_alpha': 0.07500000000000001, 'bootstrap': True}\n",
    "I continued a few more iterations of these searches, particularly focused on tuning the bootstrapping, out-of-bag error, and criterion parameters, along with doing some finer tuning of the parameters above. As the model search progressed, I began to use GridSearchCV once I felt that I was down to sufficiently few combinations of parameters to consider every possibility within a reasonable amount of time.\n",
    "Ultimately, I tuned 6 models and found that the 4th model performed best. I used the following parameters:\n",
    "{'oob_score': True, 'max_leaf_nodes': 2400, 'max_features': 4, 'max_depth': 36, 'ccp_alpha': 0.075, 'bootstrap': True, ‘criterion’: “neg_mean_absolute_error”, ‘n_estimators’: 100}\n",
    "This achieved a final MAE of $35,292 on test data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6fb3f08",
   "metadata": {},
   "source": [
    "# GradientBoosting\n",
    "*By Charles Bugayer*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d9528a",
   "metadata": {},
   "source": [
    "I tuned many, many models, starting with broad ranges (coarse) and narrowing down to slimmer ones (fine). \n",
    "The base GradientBoosting Model resulted in an MAE of 36775.60.\n",
    "For efficiency, I used RandomizedSearchCV with 100000 iterations, as a GridSearchCV would be totalling 9878400 fits with 5-fold cross validation.\n",
    "Best gbr params: {'subsample': 1.0, 'n_estimators': 160, 'min_weight_fraction_leaf': 0, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 3, 'loss': 'huber', 'learning_rate': 0.095, 'criterion': 'squared_error'} Best gbr score: -36322.8451418078\n",
    "After getting the initial best parameters, I used GridSearchCV to find the best parameters in a smaller range. After a couple of rounds of tuning, n_estimators moved to a higher value than the first round would have suggested. Because the value for min_samples_split was on the edge of the range, I increased the range.\n",
    "Best gbr params: {'learning_rate': 0.095, 'loss': 'huber', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 7, 'min_weight_fraction_leaf': 0, 'n_estimators': 189, 'subsample': 1.0} Best gbr score: -36216.57778386479\n",
    "After this round of tuning, the MAE on train data only got worse, so I decided to stop there.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37f552ef",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "*By Alex Olmeta*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeae0cca",
   "metadata": {},
   "source": [
    "Started by plotting the best values of an individual hyperparameters on a base model for a broad range\n",
    "I used this insight as a starting point for my randomized search. I used randomized search and KFold cross validation with 3 folds because of runtime constraints\n",
    "\n",
    "Broad search parameter values:\n",
    "- ‘max_depth’: [4, 6, 8, 12, 15]\n",
    "- ‘learning_rate’: [0.1, 0.05, 0.1]\n",
    "- ‘reg_lambda’: [0, 1, 10, 100]\n",
    "- ‘n_estimators’: [1000, 2000, 3000, 4000]\n",
    "- ‘gamma’: [0, 10, 100, 1000, 1e6, 1e9]\n",
    "- ‘subsample’: [0.5, 0.75, 1.0]\n",
    "\n",
    "After this I ran a further RandomSearchCV with the same number of KFolds and iterations based on a new range of the hyperparameters the last one tended towards:\n",
    "\n",
    "- ‘max_depth’: [7, 8, 9]\n",
    "- ‘learning_rate’: [0.1, 0.25, 0.05, 0.1]\n",
    "- ‘reg_lambda’: [0, 1, 10, 100]\n",
    "- ‘n_estimators’: [1000, 1500, 2000, 2500]\n",
    "- ‘gamma’: [0, 10, 100, 1000, 1e6, 1e9]\n",
    "- ‘subsample’: [0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "\n",
    "After this random search I ran many manual searches. I noticed that the ideal number of n_estimators was at the high end of the last range and the learning rate needed to be low to prevent this from causing overfitting.\n",
    "I also used the huber loss function when tuning to minimize the impact of outliers.\n",
    "\n",
    "Final hyperparameters:\n",
    "- ‘max_depth’: 7\n",
    "- ‘learning_rate’: 0.016\n",
    "- ‘Reg_lambda’: 100\n",
    "- ‘N_estimators’: 2600\n",
    "- ‘gamma’:10\n",
    "- ‘subsample’: 0.68\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be2594f1",
   "metadata": {},
   "source": [
    "## Model Ensemble "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa68b0a5",
   "metadata": {},
   "source": [
    "Put the results of enembling individual models. Feel free to add subsections in this section to add more innovative ensembling methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c5924a",
   "metadata": {},
   "source": [
    "### Voting ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a31cb2",
   "metadata": {},
   "source": [
    "The simplest voting ensemble will be the model where all models have equal weights.\n",
    "\n",
    "You may come up with innovative methods of estimating weights of the individual models, such as based on their cross-val error. Sometimes, these methods may work better than stacking ensembles, as stacking ensembles tend to overfit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcff4cda",
   "metadata": {},
   "source": [
    "### Stacking ensemble\n",
    "Try out different models as the metamodel. You may split work as follows. The person who worked on certain types of models *(say AdaBoost and MARS)* also uses those models as a metamodel in the stacking ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b22a5f3",
   "metadata": {},
   "source": [
    "### Ensemble of ensembled models\n",
    "\n",
    "If you are creating multiple stacking ensembles *(based on different metamodels)*, you may ensemble them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fe5f5f",
   "metadata": {},
   "source": [
    "### Innovative ensembling methods\n",
    "*(Optional)*\n",
    "\n",
    "Some models may do better on certain subsets of the predictor space. You may find that out, and given a data point, choose the model(s) that will best predict for that data point. This is similar to the idea of developing a decision tree metamodel. However, decision tree is prone to overfitting.\n",
    "\n",
    "Another idea may be to correct the individual models with the intercept and slope *(note the tree-based models don't have an intercept and may suffer from a constant bias)*, and then ensemble them. This is equivalent to having a simple linear regression meta-model for each of the individual models, and then ensembling the meta-models with a meta-metamodel or a voting ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b46343d",
   "metadata": {},
   "source": [
    "## Limitations of the model with regard to prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c85ead90",
   "metadata": {},
   "source": [
    "Are you confident that you found the optimal hyperparameter values for each of your individual models, and that your individual models cannot be better tuned? Or, are there any models that could be better tuned if you had more time / resources, but you are limited by the amount of time you can spend on the course project *(equivalent to one assignment)*? If yes, then which models could be better tuned and how?\n",
    "\n",
    "Will it be possible / convenient / expensive for the stakeholders to collect the data relating to the predictors in the model. Using your model, how soon will the stakeholder be able to predict the outcome before the outcome occurs. For example, if the model predicts the number of bikes people will rent in Evanston on a certain day, then how many days before that day will your model be able to make the prediction. This will depend on how soon the data that your model uses becomes available. If you are predicting election results, how many days / weeks / months / years before the election can you predict the results. \n",
    "\n",
    "When will your model become too obsolete to be useful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7823ad8",
   "metadata": {},
   "source": [
    "Our model is limited by the fact that we only have 4134 observations. This is a relatively small dataset, and we would have liked to have more observations to train our model on. We were able to predict salaries with a MAE of 35510, which we would have liked to decrease further, but we believe that this is the best we could do with the data we had. In addition, we suspect that predictors may not have been standardized across companies (for example, experience level “expert” did not mean the same across each company that provided data), which is discussed further in the conclusion section below.\n",
    "If we had more time, we’d have looked for more insights through EDA and seen if any improved our model. One of the things that we wanted to do but didn’t have the time for is group the location by country GDP. We obviously predict that countries with a higher GDP would generally provide higher salaries. We also are curious in retrospect if we should have made a predictor that checks if the company location and employee residence are the same. We also could have improved our model with additional provided predictors such as employee age.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6026cb7",
   "metadata": {},
   "source": [
    "## Other sections *(optional)*\n",
    "\n",
    "You are welcome to introduce additional sections or subsections, if required, to address any specific aspects of your project in detail. For example, you may briefly discuss potential future work that the research community could focus on to make further progress in the direction of your project's topic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "What conclusions do you draw based on your model? You may draw conclusions based on prediction accuracy, or other performance metrics.\n",
    "\n",
    "How do you use those conclusions to come up with meaningful recommendations for stakeholders? The recommendations must be action-items for stakeholders that they can directly implement without any further analysis. Be as precise as possible. The stakeholder(s) are depending on you to come up with practically implementable recommendations, instead of having to think for themselves.\n",
    "\n",
    "If your recommendations are not practically implementable by stakeholders, how will they help them? Is there some additional data / analysis / domain expertise you need to do to make the recommendations implementable? \n",
    "\n",
    "Do the stakeholder(s) need to be aware about some limitations of your model? Is your model only good for one-time use, or is it possible to update your model at a certain frequency (based on recent data) to keep using it in the future? If it can be used in the future, then for how far into the future?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ca45613",
   "metadata": {},
   "source": [
    "Add details of each team member's contribution, other than the models contributed, in the table below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5505da5c",
   "metadata": {},
   "source": [
    "<html>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "th {\n",
    "  text-align: left;\n",
    "}\n",
    "    \n",
    "\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<h2>Individual contribution</h2>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <colgroup>\n",
    "       <col span=\"1\" style=\"width: 15%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 25%;\">\n",
    "       <col span=\"1\" style=\"width: 40%;\">\n",
    "    </colgroup>\n",
    "  <tr>\n",
    "    <th>Team member</th>\n",
    "    <th>Individual Model</th>\n",
    "    <th>Work other than individual model</th>    \n",
    "    <th>Details of work other than individual model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Sylvia Sherwood</td>\n",
    "    <td>Lasso, Ridge & Catboost</td>\n",
    "    <td>Data cleaning and EDA</td>    \n",
    "    <td>Imputed missing values and visualized data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Sankaranarayanan Balasubramanian</td>\n",
    "    <td>MARS, AdaBoost & LightGBM</td>\n",
    "    <td>Ensembling</td>    \n",
    "    <td>Stacking ensembles and voting ensemble</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Fiona Fe</td>\n",
    "    <td>Bagged trees & Random forest</td>\n",
    "    <td>Variable selection</td>    \n",
    "    <td>Variable selection based on feature importance</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Ryu Kimiko</td>\n",
    "    <td>XGBoost</td>\n",
    "    <td>Ensembling</td>    \n",
    "    <td>Innovative ensemble & stacking ensemble</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b1cafe",
   "metadata": {},
   "source": [
    "## References {-}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebdb1aad",
   "metadata": {},
   "source": [
    "List and number all bibliographical references. When referenced in the text, enclose the citation number in square brackets, for example [1].\n",
    "\n",
    "[1] Authors. The frobnicatable foo filter, 2014. Face and Gesture submission ID 324. Supplied as additional material\n",
    "fg324.pdf. 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5831751c",
   "metadata": {},
   "source": [
    "## Appendix {-}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d13d374d",
   "metadata": {},
   "source": [
    "You may put additional stuff here as Appendix. You may refer to the Appendix in the main report to support your arguments. However, the appendix section is unlikely to be checked while grading, unless the grader deems it necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
